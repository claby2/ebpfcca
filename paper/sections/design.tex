\documentclass[../main.tex]{subfiles}
\begin{document}
We believe that an alternative implementation of an existing CCA can be evaluated using two metrics. First, whether or not the behavior of the implementation matches (to a reasonable extent) existing implementations of the same CCA. Secondly, the performance costs of the implementation when compared to other implementations. We included two sets of benchmarks to measure this: a behavioral benchmark aimed to verify the "correctness" of different implementations (TCP Cubic should behave similarly no matter where it is implemented, as indicated by metrics such as bitrate/congestion window size over time), and a performance benchmark aimed to measure the CPU overhead of different implementation approaches. A CCA implementation with smaller CPU overhead would logically lead to a higher maximum bandwidth in an ideal, infinite-speed connection where the CPU running the CCA is the bottleneck. \\
Our benchmarks are ran with iperf3 (cite). For each trial, we loop through every CCA we are testing and run iperf3 for 15 seconds. At the same time, we run a Python script that loops infinitely and adds 1 to an accumulator at every loop. Once the 15 seconds are up, the script then outputs the final number into a file, a higher number suggesting that the script was able to access more CPU cycles. We ran 20 trials to ensure statistical reliability. Our testing script also included various parameters that can be tuned, including trial duration, reporting intervals, and the metric to be used while generating graphs for the behavioral benchmark. \\
After all the trials, we average the behavioral metric across the trials and plot the average for each CCA through time as the behavioral benchmark. We then plot the CPU performances for each CCA across trials on a boxplot. \\

\end{document}
