\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Setup and Method}
We believe that an alternative implementation of an existing CCA can be evaluated using two metrics. First, whether or not the behavior of the implementation matches (to a reasonable extent) existing implementations of the same CCA. Secondly, the performance costs of the implementation when compared to other implementations. We included two sets of benchmarks to measure this: a behavioral benchmark aimed to verify the "correctness" of different implementations (TCP Cubic should behave similarly no matter where it is implemented, as indicated by metrics such as bitrate/congestion window size over time), and a performance benchmark aimed to measure the CPU overhead of different implementation approaches. A CCA implementation with smaller CPU overhead would logically lead to a higher maximum bandwidth in an ideal, infinite-speed connection where the CPU running the CCA is the bottleneck. \\
Our benchmarks are ran with iperf3 \cite{iperf}. For each trial, we loop through every CCA we are testing and run iperf3 for 15 seconds. At the same time, we run a Python script that loops infinitely and adds 1 to an accumulator at every loop. Once the 15 seconds are up, the script then outputs the final number into a file, a higher number suggesting that the script was able to access more CPU cycles. We ran 20 trials to ensure statistical reliability. Our testing script also included various parameters that can be tuned, including trial duration, reporting intervals, and the metric to be used while generating graphs for the behavioral benchmark. \\
After all the trials, we average the behavioral metric across the trials and plot the average for each CCA through time as the behavioral benchmark. We then plot the CPU performances for each CCA across trials on a boxplot. \\

In order to make sure we are correctly benchmarking the performance impact of CCAs rather than miscellneous confounding factors, we ran the benchmark on an out-of-the-box Debian image on a VM. To ensure the CPU benchmark and the CCA are running on the same CPU, our VM is set up to contain only one core. We also decided to run our benchmarks on various implementations of TCP Cubic due to its wide availability across platforms.

\subsection{Results}
The results of behavioral benchmarks are shown in Figure \ref{fig:behavioral}. From the graph, we can see that [to-be-added-conclusion]. The results of CPU benchmarks are shown in Figure \ref{fig:cpu}. The eBPF implementation of TCP Cubic achieved impressive CPU performance compared to the built-in Linux kernel networking stack implementation. CCP-Cubic from the CCP Project \cite{generic-cong-avoid} combined with the kernel module + netlink CCP datapath achieved slightly lower performance, where its median is at x percent of the full kernel implementation value. Our implementation of eBPFCCP ranked lower on the performance chart, but was still able to achieve y percent of the full kernel implementation value. Due to the relative comparability of CPU performance between the kernel and eBPF implementations of TCP Cubicm, we theorize the noticable gap between eBPFCCP and kernel module CCP was due to our choice of using unix pipes rather than netlink as a channel for communicating between the eBPFCCP and the user space program. \\
From these results, we can conclude that eBPF is a viable platform for the development CCAs. The negligible performance overhead when compared to the kernel native implementation is impressive, and suggests that eBPF-CCAs can possibly also be used as a long-term deployment option outside of research and development stages. 
\end{document}
