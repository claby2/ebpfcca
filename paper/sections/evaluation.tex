\documentclass[../main.tex]{subfiles}
\begin{document}
We included two sets of benchmarks: a behavioral benchmark aimed to verify the "correctness" of different implementations (TCP Cubic should behave similarly no matter where it is implemented, as indicated by metrics such as bitrate/congestion window size over time), and a performance benchmark aimed to measure the CPU overhead of different implementation approaches. A CCA implementation with smaller CPU overhead would logically lead to a higher maximum bandwidth in an ideal, infinite-speed connection where the CPU running the CCA is the bottleneck. \\
Our benchmarks are ran with iperf3 (cite). For each trial, we loop through every CCA we are testing and run iperf3 for 15 seconds. At the same time, we run a Python script that loops infinitely and adds 1 to an accumulator at every loop. Once the 15 seconds are up, the script then outputs the final number into a file, a higher number suggesting that the script was able to access more CPU cycles. We run 20 trials to ensure statistical reliability. \\
After all the trials, we average the behavioral metric across the trials and plot the average for each CCA through time as the behavioral benchmark. We then plot the CPU performances for each CCA across trials on a boxplot. In order to make sure we are correctly benchmarking the performance impact of CCAs instead of confounding factors, we ran the benchmark on an out-of-the-box Debian images on a VM. To ensure the CPU benchmark and the CCA are running on the same CPU, our VM is set up to contain only one core. [Results and interpretations]
\end{document}
